{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11장 텍스트를 위한 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 자연어 처리 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 텍스트 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1 텍스트 표준화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2 텍스트 분할(토큰화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3 어휘 사전 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = {}\n",
    "# for text in dataset:\n",
    "#     text = standardize(text)\n",
    "#     tokens = tokenize(text)\n",
    "#     for token in tokens:\n",
    "#         if token not in vocabulary:\n",
    "#             vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot_encode_token(token):\n",
    "#     vector = np.zeros((len(vocabulary),))\n",
    "#     token_index = vocabulary[token]\n",
    "#     vector[token_index] = 1\n",
    "#     return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.4 TextVectorization 층 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not int string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items()\n",
    "        )\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence\n",
    "        )\n",
    "\n",
    "vetorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
